---
title: "📘 하둡 에코시스템과 운영 및 관리"
tags:
    - Apache
    - Hadoop
    - Study
date: "2025-04-05"
thumbnail: "/assets/img/thumbnail/hadoop_basic_3.png"
bookmark: true
---

저번 포스팅에서는, Map Reduce 를 공부하였습니다.
이번에는 하둡의 여러 에코시스템들에 대해 공부하고 정리해보도록 하겠습니다.

<br>
> 📌 Hadoop 을 공부하고 정리한 파트이기 때문에, 에코시스템에 대한 기본적인 내용을 담고 있습니다. 각 에코시스템의 디테일한 내용은 추후 공부하고 따로 포스팅하겠습니다. :D

<br>
[📘 분산 시스템의 이해와 하둡의 등장 배경](https://jeondaehong.github.io/%F0%9F%93%96%20%EA%B0%9C%EC%9D%B8%20%EA%B3%B5%EB%B6%80/%F0%9F%91%89%20Apache%20Hadoop/1.%20%EB%B6%84%EC%82%B0%20%EC%8B%9C%EC%8A%A4%ED%85%9C%EC%9D%98%20%EC%9D%B4%ED%95%B4%EC%99%80%20%ED%95%98%EB%91%A1%EC%9D%98%20%EB%93%B1%EC%9E%A5%20%EB%B0%B0%EA%B2%BD.html)
[📘 하둡의 핵심 구성요소와 이론](https://jeondaehong.github.io/%F0%9F%93%96%20%EA%B0%9C%EC%9D%B8%20%EA%B3%B5%EB%B6%80/%F0%9F%91%89%20Apache%20Hadoop/2.%20%ED%95%98%EB%91%A1%EC%9D%98%20%ED%95%B5%EC%8B%AC%20%EA%B5%AC%EC%84%B1%EC%9A%94%EC%86%8C%EC%99%80%20%EC%9D%B4%EB%A1%A0.html)
[📘 YARN(Yet Another Resource Negotiator) 기본 이론](https://jeondaehong.github.io/%F0%9F%93%96%20%EA%B0%9C%EC%9D%B8%20%EA%B3%B5%EB%B6%80/%F0%9F%91%89%20Apache%20Hadoop/3.%20YARN(Yet%20Another%20Resource%20Negotiator)%20%EA%B8%B0%EB%B3%B8%20%EC%9D%B4%EB%A1%A0.html)
[📘 Map Reduce 기본 이론](https://jeondaehong.github.io/%F0%9F%93%96%20%EA%B0%9C%EC%9D%B8%20%EA%B3%B5%EB%B6%80/%F0%9F%91%89%20Apache%20Hadoop/4.%20Map%20Reduce%20%EA%B8%B0%EB%B3%B8%20%EC%9D%B4%EB%A1%A0%EA%B3%BC%20%EC%8B%A4%EC%8A%B5.html)

<br>
<div align="center">◈</div>
<br>

# 🐘 1. 하둡 에코시스템과 관련 프로젝트

하둡은 단순한 분산 저장 및 처리 프레임워크로 출발했지만, 현실 세계의 복잡한 데이터 처리 요구사항을 만족시키기 위해 수많은 에코시스템 구성 요소들로 확장되었습니다.
이러한 에코시스템들은 하둡의 **기본적인 기능(HDFS와 MapReduce)**만으로는 해결하기 어려운 실질적인 문제들을 보완하고, 다양한 사용 시나리오에 대응하기 위해 등장하게 되었습니다.

옛날로 돌아가보면, 초기 하둡은 HDFS를 통해 데이터를 저장하고, MapReduce를 이용해 데이터를 처리하는 구조로 시작되었습니다.
이 구조는 대규모 배치 처리에는 적합했지만, 여러가지 제한점이 존재했습니다.

몇 가지 예시를 들어보면,
MapReduce는 배치 처리에 최적화되어 있어, 실시간 로그 분석, 이벤트 기반 처리등 스트리밍 데이터나 실시간 분석에는 적합하지 않았으며,
모든 처리를 Java 기반으로 MapReduce 로직을 작성해야 했기 때문에, SQL을 선호하는 데이터 분석가들에게는 진입장벽이 매우 높았습니다.
또한, 배치 처리 외에 ETL, 머신러닝, 검색, 그래프 분석 등 다양한 워크로드를 유연하게 지원하기 어려웠습니다.

이러한 한계점을 보완하기 위해, 하둡 기반 위에 전문적인 기능을 수행하는 다양한 서브 프로젝트들이 자연스럽게 등장했습니다.

하둡은 하나의 거대한 일체형 소프트웨어가 아니라, 서로 다른 역할을 수행하는 여러 프로젝트들이 **느슨하게 결합된 모듈형 생태계(Modular Ecosystem)**로 구성되어 있습니다.

각각의 도구들은 특정 문제 영역을 해결하도록 설계되었고, 서로 연동되면서 보다 유연하고 확장 가능한 분산 시스템을 구성합니다.

아래에서 더 자세히 알아보겠지만, 지금 몇 가지만 간략하게 이야기해보면 이해가 쉬울겁니다.

- **Pig, Hive** → SQL 혹은 DSL로 MapReduce 추상화 → 사용자 편의성 향상

- **HBase** → HDFS 위에 컬럼 기반 DB 구축 → 실시간 조회 기능 제공

- **Zookeeper** → 분산 환경에서의 구성 관리 및 락 처리 → 고가용성 보장

- **Oozie, Airflow** → 워크플로우와 작업 스케줄링 → 자동화된 데이터 파이프라인

- **YARN** → 자원 관리의 중앙 집중화 → 다양한 애플리케이션 실행 지원

- **Spark, Tez, Flink** → MapReduce를 대체하거나 보완하는 고속 처리 엔진

이처럼 각각의 프로젝트는 독립적으로도 활용 가능하지만, 하둡이라는 공통된 기반 위에서 상호 보완적인 관계로 작동합니다. 이러한 설계는 하둡이 단순한 프레임워크를 넘어, 범용 분산 데이터 처리 플랫폼으로 진화하게 된 핵심 배경이기도 합니다.

위와 같은 이유와 사상을 가지고, 하둡 에코시스템은 오픈소스를 기반으로 하며, 누구나 참여하고 확장할 수 있도록 설계되어 있습니다. 이로 인해 다양한 기업과 커뮤니티에서 자신들의 요구에 맞춰 하둡을 커스터마이징하고, 새로운 컴포넌트를 추가할 수 있게 되었습니다.

대표적인 예로, Cloudera, Hortonworks, MapR 등 상용 디스트리뷰터들은 하둡을 기반으로 한 통합 플랫폼을 제공하면서, 보안, 모니터링, 통합 관리 도구 등 엔터프라이즈 환경에서 필요한 기능들을 추가해 생태계를 더욱 풍성하게 만들었습니다.

그럼 지금부터 대표적인 에코시스템 몇 가지를 알아보도록 하겠습니다.

<br>
## 🐘 1.1. Apache Avro 와 Apache Parquet
---


## 🐘 1.2. Apache Flume
---


## 🐘 1.3. Apache Sqoop
---


## 🐘 1.4. Apache Kerberos
---


## 🐘 1.5. Apache Hive
---


## 🐘 1.6. Apache Impala
---


## 🐘 1.7. Apache HBase
---


## 🐘 1.8. Apache Spark
---


## 🐘 1.9. Apache Oozie
---


## 🐘 1.10. Apache Zookeeper
---


## 🐘 1.11. Apache Crunch
---


## 🐘 1.12. 그 외 프로젝트와, 프로젝트의 큰 종류
---




<br>
<br>
<div align="center">◈</div>
<br>

# 🐘 2. 하둡의 운영과 관리

## 🐘 2.1. 하둡의 주요 설정 파일과, 설정 값
---

## 🐘 2.2. 실제 하둡 클러스터 구축과 보안
---

## 🐘 2.3. 현업에서의 하둡을 관리하는 방법
---

<br>
<br>
<div align="center">◈</div>
<br>

# 🐘 3. 기타 하둡에 대해 알아야 할 몇 가지

## 🐘 3.1. 하둡 I/O
---

## 🐘 3.2. Schema on Read vs Schema on Write
---

## 🐘 3.3. 하둡의 세 가지 모드
---

## 🐘 3.4. CDH
---

## 🐘 3.5. Cascading
---

## 🐘 3.6. 쿠버네티스 환경에서는 하둡이 되지 않는 이유?
---

<br>
<br>
<div align="center">◈</div>
<br>

# 🐘 4. 실제 하둡 관련 면접 질문 사례 모음

## ❓ 질문: Hadoop을 사용하는 데 있어 흔히 발생하는 과제는 무엇인가요?
---
**✅ 1. NameNode의 단일 장애 지점 문제**
Hadoop의 초기 구조에서는 NameNode가 클러스터 전체의 메타데이터를 단독으로 관리합니다.
따라서 NameNode가 장애를 일으키면 전체 클러스터의 운영이 중단되고, 최악의 경우 데이터 손실이 발생할 수 있습니다.
이를 보완하기 위해 High Availability(HA) 구성이 도입되었지만, 기본 구조 자체가 갖는 위험성은 여전히 존재합니다.

**✅ 2. 보안 취약성**
기본 Hadoop은 보안 기능이 제한적입니다. 그래서 데이터 접근 제어가 세분화되어 있지 않고, 누가 데이터를 조회하거나 변경했는지 추적하기 어렵습니다.
Kerberos 같은 인증 시스템이 있지만, 설정이 복잡하고 유지 비용이 크다는 단점이 있습니다.
추가 보안 계층(Sentry, Ranger 등)을 도입해야 실질적인 데이터 보호가 가능하지만, 이는 운영 복잡도를 높이는 요인이 됩니다.

**✅ 3. 실시간 처리 미지원**
Hadoop의 핵심 처리 모델인 MapReduce는 배치 처리 기반입니다.
이 방식은 대용량 로그 분석 등에는 효과적이지만, 실시간 스트리밍 데이터 처리에는 부적합합니다.
그래서 Spark, Flink 같은 별도의 실시간 처리 프레임워크가 이를 보완합니다.

**✅ 4. 작은 파일 처리 비효율**
Hadoop은 기본적으로 대용량 데이터를 다루는 데 최적화되어 있습니다.
각 파일마다 메타데이터가 NameNode에 저장되며, 파일 수가 많아질수록 NameNode 메모리 사용량 급증합니다.
결과적으로 작은 파일들의 갯수가 늘어나면 응답 지연 및 성능 저하가 발생하게 됩니다.
이를 해결하기 위해 Hadoop Archive 등의 접근이 필요합니다.

<br>
## ❓ 질문: Hadoop에서 분산 캐시(Distributed Cache)의 목적은 무엇인가요?
---
Hadoop의 **분산 캐시(Distributed Cache)**는 MapReduce 작업 실행 시 모든 노드에 필요한 읽기 전용 파일(예: JAR 파일, 설정 파일, 참조용 작은 데이터 파일 등)을 미리 로컬에 배포하여 성능을 향상시키는 기능입니다.
HDFS에서 반복적으로 읽는 대신, 작업 시작 시 노드 로컬 디스크에 캐시되어 파일을 반복 다운로드하지 않도록 방지하고,
작은 참조 파일을 빠르게 읽을 수 있어 처리 속도가 개선됩니다.
또한 Mapper/Reducer가 동일한 참조 파일을 활용 할 수 있게 되기 때문에 성능이 향상됩니다.
예를 들어, MapReduce 작업 40개가 있고, 각 작업이 동일한 작은 참조 파일을 반복적으로 읽어야 한다면, 이 파일이 HDFS에만 존재할 경우 매번 네트워크 I/O가 발생합니다.
하지만 분산 캐시에 등록하면 해당 파일이 각 노드 로컬 디스크에 복사되어, 모든 작업이 빠르게 접근할 수 있습니다.

<br>
## ❓ 질문: Hadoop에서 "데이터 지역성(Data Locality)"이란 무엇인가요?
---
Hadoop에서 데이터 지역성이란, 데이터를 처리할 때 데이터를 네트워크를 통해 다른 위치로 옮기지 않고, 계산 작업(Task)을 데이터가 저장된 위치로 이동시키는 방식을 의미합니다. 이는 대용량 데이터를 다루는 환경에서 네트워크 전송 비용을 줄이고 성능을 높이기 위한 핵심 개념입니다.
빅데이터 환경에서는 처리해야 할 데이터의 양이 매우 방대하기 때문에, 데이터를 네트워크를 통해 전송하는 데 걸리는 시간과 비용이 상당히 큽니다. 따라서 Hadoop은 데이터를 이동시키는 대신, 계산을 데이터가 위치한 서버(데이터 노드)에서 수행함으로써 전체 처리 시간을 줄이고 시스템 효율을 높입니다.
Hadoop의 작업 스케줄러는 다음과 같은 우선순위 규칙에 따라 작업을 스케줄링합니다
- **로컬(local):** 먼저, 데이터가 저장된 같은 노드에서 작업을 실행할 수 있는지 확인합니다. 이것이 가장 이상적인 경우입니다.
- **랙(local rack):** 만약 해당 노드에 작업을 실행할 수 없다면, 같은 랙(Rack) 내에 있는 다른 노드에서 실행을 시도합니다. 같은 랙은 네트워크 지연이 적기 때문입니다.
- **원격(other rack):** 위의 방법들이 모두 불가능할 경우, 다른 랙에 위치한 복제본이 저장된 노드에서 작업을 실행합니다.
예를들면, 어떤 회사에서 고객 로그 데이터를 분석하기 위해 Hadoop 클러스터를 사용하고 있다고 가정해보겠습니다. 이 로그 데이터는 logs.csv라는 이름으로 HDFS에 저장되어 있으며, 총 300GB 크기로 세 노드에 분산 저장되어 있습니다.
Node1에는 logs.csv의 블록 1이 저장되어 있고, Node2에는 블록 2, Node3에는 블록 3이 저장되어 있습니다. 각 블록은 복제(replication factor = 3) 되어, 세 노드 중 두 곳에 추가로 복사본이 존재합니다.
Hadoop은 먼저 각 블록을 저장하고 있는 노드에서 해당 블록을 처리하는 작업을 실행하려고 합니다. 예를 들어, Block 1을 처리하는 작업은 우선 Node1에서 실행됩니다. 이 경우, 데이터는 디스크에서 바로 읽히므로 네트워크를 사용하지 않습니다.
만약 Node1의 리소스가 부족해서 작업을 실행할 수 없다면, Hadoop은 Block 1의 복제본이 있는 다른 노드에서 실행을 시도합니다. 예를 들어, Block 1의 복제본이 Node2에도 있다면, Hadoop은 Node2에서 작업을 실행합니다. 이 경우에도 여전히 로컬 디스크 접근이 가능하므로 성능이 좋습니다.
만약 Block 1을 가진 어떤 노드에서도 작업을 실행할 수 없다면, Hadoop은 같은 랙(rack) 내의 다른 노드에서 작업을 실행합니다. 이 경우, 데이터는 네트워크를 통해 전달되지만, 랙 내부 통신은 상대적으로 빠르기 때문에 손실이 크지 않습니다.
가장 나쁜 경우는, 해당 데이터를 가진 노드가 모두 바쁘고, 다른 랙에 있는 노드에서 데이터를 네트워크로 받아와야 할 때입니다. 이 경우는 네트워크 병목이 발생할 수 있고, 성능 저하로 이어집니다.

<br>
<br>
<div align="center">◈</div>
<br>

# 🐘 5. 점차 사라지는 하둡, 그러나 하둡을 얕게라도 공부해야 하는 이유
---

<br>
<br>
<div align="center">◈</div>
<br>

# ✏️ 결론
---

<br>
<br>
<div align="center">◈</div>
<br>

# 📚 공부 참고 자료
---