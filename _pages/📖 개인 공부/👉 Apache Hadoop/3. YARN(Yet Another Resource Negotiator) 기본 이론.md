---
title: "📘 YARN(Yet Another Resource Negotiator) 기본 이론"
tags:
    - Apache
    - Hadoop
    - Study
date: "2025-03-29"
thumbnail: "/assets/img/thumbnail/hadoop_basic_5.png"
bookmark: true
---

저번 포스팅에서는, 하둡의 핵심을 공부하였습니다.
이번에는 Hadoop V2 의 핵심인 YARN 에 대하여 공부한 내용을 포스팅 하도록 하겠습니다.

[📘 분산 시스템의 이해와 하둡의 등장 배경](https://jeondaehong.github.io/%F0%9F%93%96%20%EA%B0%9C%EC%9D%B8%20%EA%B3%B5%EB%B6%80/%F0%9F%91%89%20Apache%20Hadoop/1.%20%EB%B6%84%EC%82%B0%20%EC%8B%9C%EC%8A%A4%ED%85%9C%EC%9D%98%20%EC%9D%B4%ED%95%B4%EC%99%80%20%ED%95%98%EB%91%A1%EC%9D%98%20%EB%93%B1%EC%9E%A5%20%EB%B0%B0%EA%B2%BD.html)
[📘 하둡의 핵심 구성요소와 이론](https://jeondaehong.github.io/%F0%9F%93%96%20%EA%B0%9C%EC%9D%B8%20%EA%B3%B5%EB%B6%80/%F0%9F%91%89%20Apache%20Hadoop/2.%20%ED%95%98%EB%91%A1%EC%9D%98%20%ED%95%B5%EC%8B%AC%20%EA%B5%AC%EC%84%B1%EC%9A%94%EC%86%8C%EC%99%80%20%EC%9D%B4%EB%A1%A0.html)

<br>
<div align="center">◈</div>
<br>

# 🐘 1. YARN

2012년 이전, Hadoop을 이용해 대용량 처리를 하려면 MapReduce 모델을 기반으로 Java, Python, Ruby, 혹은 Pig 프레임워크를 사용해 개발해야 했습니다.  

하지만 Hadoop 2.0과 함께 등장한 Yarn은 MapReduce 프로그래밍 모델의 제약에서 벗어나, 다양한 멀티프로세싱 프로그램을 Hadoop 자원을 활용해 자유롭게 실행할 수 있게 해주었습니다.  

Yarn은 대용량 멀티프로세싱 처리에서 성능 향상과 유연한 실행 엔진(execution engine)을 제공한다는 점이 큰 장점입니다.

또한, 시간이 지나면서 Spark와 같은 다양한 분산 처리 프레임워크도 Yarn을 지원하게 되면서, 하둡 생태계에서 필수적인 리소스 관리 플랫폼으로 자리잡았습니다.

이러한 **YARN** 에 대하여, 지금부터 공부하고, 정리한 내용을 포스팅 해보도록 하겠습니다.

## 🐘 1.1. YARN 이란
---

<img src="/assets/img/yarn.png" alt="yarn" style="border: 2px solid skyblue; border-radius: 4px;" />

> 출처 : [YARN](https://bigdataschool.ru/blog/high-availability-of-hadoop-cluster-with-yarn.html)

<br>
Yarn은 Hadoop 2.0부터 도입된 클러스터 자원 관리 시스템(Resource Manager)입니다. 
이전 Hadoop 버전에서는 MapReduce라는 특정 프로그래밍 모델에만 의존해서 작업을 처리했는데, Yarn이 도입되면서 MapReduce에 국한되지 않고 다양한 종류의 분산 처리 작업을 실행할 수 있게 되었습니다.  

Yarn의 주요 역할은 클러스터 내 여러 노드의 자원을 효율적으로 관리하고 할당하여, 여러 애플리케이션이 동시에 안정적으로 실행될 수 있도록 조율하는 것입니다.  
즉, 여러 사용자가 다양한 작업을 동시에 실행해도 자원이 과도하게 집중되거나 낭비되지 않고, 적절하게 분배되어 전체 시스템의 효율성과 안정성을 높입니다.

Yarn은 또한 실행 환경을 유연하게 제공하여, Apache Spark, Flink, Tez 같은 다양한 분산 처리 프레임워크도 지원합니다.
이로 인해 빅데이터 처리의 범위가 단순한 배치 작업에서 실시간 스트리밍, 머신러닝, 그래프 처리 등으로 확장될 수 있었습니다.

<br>
### 🐘 1.1.1. Yarn의 대표적인 Use-Cases

Yarn이 등장하면서 Hadoop 기반 클러스터는 다음과 같은 장점과 가능성을 갖게 되었습니다.

- **🔍 배치 작업뿐 아니라 반복 작업과 실시간 스트리밍 처리 지원**  
  과거 Hadoop은 일괄 처리(batch processing)에 초점이 맞춰져 있었으나, Yarn 덕분에 지속적으로 데이터를 처리하는 스트리밍 작업과 주기적으로 반복 실행되는 작업들도 효율적으로 수행할 수 있게 되었습니다.  
  예를 들어, 실시간 로그 분석, 실시간 추천 시스템 등 웹 서비스에 필요한 빠른 데이터 처리가 가능해졌습니다.

- **🔍 클러스터 자원 활용률 극대화**  
  클러스터 내 자원을 한 애플리케이션이 모두 사용하지 않을 때, Yarn이 자원을 다른 애플리케이션에 재할당해 줌으로써 자원 활용률을 크게 향상시켰습니다.  
  이는 비용 절감과 클러스터 운영 효율을 높이는 데 중요한 역할을 합니다.

- **🔍 통합 클러스터 운영**  
  데이터 저장(HDFS), 데이터 처리(MapReduce, Spark 등), 데이터 조회 등 다양한 작업을 단일 클러스터에서 함께 처리할 수 있어 운영 관리가 간편해졌고, 데이터 이동에 필요한 시간과 비용도 줄일 수 있었습니다.

- **🔍 다양한 애플리케이션 동시 실행 지원**  
  한 클러스터에서 여러 종류의 애플리케이션이 동시에 안정적으로 실행되어, 여러 팀이나 서비스가 하나의 클러스터를 공유할 수 있습니다.
  
<br>
### 🐘 1.1.2. Container 란?

Docker 를 사용해보신 분들이라면, Container 의 개념을 어느정도 알고 계실 것입니다.

컨테이너는 소프트웨어 실행 환경을 하나의 패키지로 묶은 경량화된 가상화 기술입니다.  
여기에는 애플리케이션이 동작하는 데 필요한 코드, 라이브러리, 설정 파일 등이 포함되어 있어, 어디서든 동일한 환경에서 실행할 수 있습니다.

컨테이너 기술은 리눅스의 cgroup과 namespace 같은 기능을 활용하여, 하나의 물리적 서버 내에서도 각각의 컨테이너가 독립적으로 CPU, 메모리, 네트워크 자원을 사용할 수 있도록 격리합니다.  
이 격리 덕분에 서로 다른 컨테이너가 충돌하거나 간섭하지 않고, 안정적으로 동시에 실행될 수 있습니다.

컨테이너를 사용하는 주요 이유와 장점은 다음과 같습니다

- **📦 책임 분리(Separation of responsibility)**  
  개발자는 애플리케이션 코드와 그에 필요한 의존성만 신경 쓰면 되고, 배포 환경의 세부 사항(운영체제 버전, 라이브러리 충돌 등)은 크게 신경 쓸 필요가 없습니다.  
  덕분에 개발과 운영 간 협업이 원활해지고, 배포 및 테스트 과정에서 환경 문제로 인한 오류가 줄어듭니다.

- **📦 높은 이식성(Portability)**  
  컨테이너는 운영체제 수준에서 가상화되므로, 리눅스 기반 서버라면 어느 환경에서든 동일하게 실행할 수 있습니다.  
  단, 컨테이너가 정상적으로 작동하려면 해당 시스템이 컨테이너 실행을 지원해야 하며, 일반적으로 Linux/amd64 아키텍처에서 가장 원활합니다.

- **📦 애플리케이션 격리(Application isolation)**  
  컨테이너마다 CPU, 메모리, 스토리지, 네트워크 같은 자원을 할당하고 제한할 수 있기 때문에, 하나의 서버에서 여러 컨테이너가 동시에 실행되어도 서로 간섭 없이 안정적으로 작동합니다.  
  이는 시스템 안정성과 보안성을 높이는 데도 도움이 됩니다.

최근에는 **Kubernetes** 같은 오케스트레이션 도구와 함께 컨테이너를 사용해, 대규모 분산 시스템을 효율적으로 운영하는 사례가 많아지고 있습니다.  

**Yarn**도 이런 컨테이너 개념을 차용해 클러스터 자원 관리에 적용함으로써, 더욱 세밀하고 유연한 자원 할당과 작업 관리를 가능하게 합니다.


## 🐘 1.2. YARN 아키텍처
---

<img src="/assets/img/yarn2.png" alt="HDFS_and_YARN" style="border: 2px solid skyblue; border-radius: 4px;" />

> 출처 : [YARN 과 HDFS](https://seoyeonhwng.medium.com/hadoop-yarn-%EC%9D%B4%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80-3f209e901b3a)

<br>
Yarn 과 HDFS 자체는 완전히 독립적입니다.

Yarn은 CPU, 메모리와 같은 컴퓨팅 자원을 할당하고 관리하는 **자원 관리 소프트웨어**입니다.반면, HDFS는 분산 파일 시스템으로 **데이터 저장(스토리지)** 역할만 수행합니다. 따라서 **Yarn과 HDFS는 완전히 독립적인 시스템**이며, 서로 다른 역할을 맡고 있습니다. 

즉, Yarn은 저장소와 관계없이 클러스터의 컴퓨팅 자원 관리에 집중합니다.

그러한 Yarn 의 Architecture 는 크게 3가지 역할을 하는 컴포넌트로 구성 되어있습니다.

 - **Resource Manager:** 클러스터 전체 자원 관리 및 스케줄링

 - **Application Master:** 개별 애플리케이션의 자원 요청 및 관리

 - **Node Managers:** 각 노드에서 자원 할당 및 작업 실행

<img src="/assets/img/yarn3.png" alt="YARN_Architecture" style="border: 2px solid skyblue; border-radius: 4px;" />

> 출처 : [YARN 아키텍처](https://velog.io/@me529/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81-Part8-Ch3.-Yarn)

<br>
그럼 YARN 의 기본과 핵심 아키텍처를 하나하나 알아보도록 하겠습니다.

<br>
### 🐘 1.2.1. Resource Mananger (RM)
Resource Manager(RM)는 YARN 클러스터에서 가장 중요한 마스터 컴포넌트입니다.  

클러스터 내 모든 컴퓨팅 자원(CPU, 메모리 등)을 총괄 관리하는 **관리 본부** 역할을 합니다.  

예를 들어, 회사 내 여러 부서가 공유하는 컴퓨터 자원을 중앙에서 효율적으로 배분하는 총무팀이라고 생각할 수 있습니다.
 
RM은 클러스터 내에 존재하는 모든 노드들의 상태와 자원 정보를 알고 있으며, Rack Awareness(서로 물리적으로 가까운 서버 그룹 간의 네트워크 우선순위)도 고려해 자원 할당을 최적화합니다.  

RM 내부에는 여러 서비스가 존재하지만, 그 중에서도 Scheduler가 핵심입니다. Scheduler는 각 애플리케이션이 요청한 자원 요구사항을 분석해, 누가 언제 얼마나 자원을 쓸지 결정합니다.  

대표적인 Scheduler로는 Capacity Scheduler, Fair Scheduler 등이 있습니다. 이들은 클러스터 자원을 공평하게 나누거나, 우선순위에 따라 할당하는 정책을 구현합니다. 이렇게 RM은 단순히 자원을 할당하는 역할뿐 아니라, 리소스 할당 정책을 적용하고, 자원 사용 현황을 모니터링하며 장애 대응을 위한 정보도 관리합니다.  

<br>
### 🐘 1.2.2. Application Mananger (AM)
Application Master(AM)는 YARN에서 실행되는 각각의 애플리케이션에 대해 별도로 실행되는 매니저 프로세스입니다.  

클러스터 전체를 관리하는 RM과 달리, AM은 특정 애플리케이션 단위의 **프로젝트 팀장** 역할을 합니다.

애플리케이션이 시작될 때 가장 먼저 실행되는 컨테이너(Container)로서, 자신의 작업에 필요한 리소스를 RM에 요청하고 할당받은 자원을 활용해 작업을 수행합니다.
  
예를 들어, **데이터 분석 작업 팀**의 팀장이라면, RM에게 ‘CPU 4개, 메모리 8GB를 달라’고 요청하고, 할당받으면 그 자원을 기반으로 작업을 진행하는 식입니다.

AM은 할당받은 자원 내에서 작업 단위를 관리하고, 필요에 따라 추가 자원 요청, 작업 상태 모니터링, 오류 처리 등을 담당합니다. AM이 실패하면 해당 애플리케이션 작업 자체가 실패할 수 있으므로 안정성과 복구 메커니즘도 중요합니다. 또한 AM은 각기 다른 분산 처리 프레임워크(예: MapReduce, Spark, Tez 등)에 맞게 구현되며, 프레임워크별로 자원 관리 및 작업 조정 방법이 다를 수 있습니다. 

<br>
### 🐘 1.2.3. Node Manager
Node Manager 는 클러스터 내 여러 노드(서버)에서 실행되는 에이전트입니다. 
 
각 Node Manager 는 ‘서버 담당 기술자’로 비유할 수 있으며, 해당 노드의 상태와 리소스를 RM에 주기적으로 보고(heartbeat)합니다.  

Node Manager 는 CPU 코어 수, 사용 가능한 메모리 양 등 자신의 노드 자원 용량(capacity)을 관리합니다. 

YARN Scheduler가 할당을 결정하면 NM은 해당 자원 일부를 컨테이너(Container) 단위로 배정하고, 클라이언트가 제출한 작업을 실행합니다. 
 
컨테이너는 리소스 격리를 위해 독립적인 실행 공간을 제공하며, CPU, 메모리, 네트워크 사용량을 제한할 수 있습니다. 또한 NM은 자원 사용량 모니터링, 컨테이너 시작 및 종료, 장애 감지와 복구 등 작업을 수행합니다.  

Node Manager 는 Resource Mananger 의 지시에 따라 컨테이너 상태를 주기적으로 보고하며, RM은 이를 통해 클러스터 자원 현황을 정확히 파악할 수 있습니다.  

NodeManager 내부에는 Container Manager, NodeStatusUpdater, NodeHealthChecker 등의 서비스가 있어, 각각 컨테이너 관리, 상태 업데이트, 노드 건강 체크 기능을 수행합니다.  

이처럼 NM은 클러스터 내 물리적 하드웨어 자원을 추상화하여 YARN 애플리케이션에 안정적으로 제공하는 역할을 합니다.

<br>
## 🐘 1.3. YARN 의 작업 흐름
---

<img src="/assets/img/yarn4.png" alt="YARN" style="border: 2px solid skyblue; border-radius: 4px;" />

> 출처 : [YARN 전체적인 흐름](https://velog.io/@me529/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81-Part8-Ch3.-Yarn)

작업의 흐름을 먼저 간단히 정리하고, 세부적으로 들여다보겠습니다.

1. **클라이언트가 어플리케이션 실행 요청을 보냅니다.**  
   - 사용자가 작업을 실행하고 싶을 때, 클라이언트 프로그램이 Yarn API를 통해 실행 요청을 합니다.  
   - ResourceManager가 이 요청을 받고 유효하다고 판단하면, 클라이언트에게 고유한 Application ID(작업 식별 번호)를 할당합니다.

2. **ResourceManager가 NodeManager에게 Application Master 실행을 요청합니다.**  
   - ResourceManager는 클러스터 내 노드들 중 하나의 NodeManager에게 Application Master라는 특별한 프로그램을 실행하라고 지시합니다.  
   - Application Master는 해당 어플리케이션의 작업 진행을 총괄 관리하는 역할을 합니다.

3. **NodeManager가 컨테이너 안에서 Application Master를 실행합니다.**  
   - NodeManager는 ResourceManager의 요청을 받고, 새 컨테이너(작업 실행 공간)를 만들어 JVM(Java Virtual Machine)을 실행하여 Application Master를 띄웁니다.

4. **Application Master가 ResourceManager에게 필요한 리소스를 요청합니다.**  
   - Application Master는 작업에 필요한 리소스(메모리, CPU, 네트워크, 컨테이너 개수 등)를 ResourceManager에 요청합니다.  
   - ResourceManager는 클러스터 전체 리소스 상황을 파악해, 사용할 수 있는 NodeManager 목록을 Application Master에 전달합니다.

5. **Application Master가 할당받은 NodeManager에 컨테이너 실행을 요청합니다.**  
   - 받은 NodeManager들에게 실제 작업을 수행할 컨테이너 실행을 요청합니다.

6. **NodeManager들이 컨테이너를 실행하고 어플리케이션을 동작시킵니다.**  
   - NodeManager들은 각 컨테이너 안에 JVM을 새로 띄워 어플리케이션을 실행합니다.  
   - 작업이 끝나면 Application Master도 종료되고, ResourceManager는 종료된 Application Master가 사용하던 자원을 해제합니다.

<br>
이제 좀 더 세부적으로 들여다보겠습니다.

### 🐘 1.3.1. client ▸ RM : Application 실행 요청

<img src="/assets/img/yarn5.png" alt="YARN" style="border: 2px solid skyblue; border-radius: 4px;" />

> 출처 : [YARN Application 실행 요청 단계](https://velog.io/@me529/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81-Part8-Ch3.-Yarn)

<br>
YARN 클러스터에서 어플리케이션을 실행하려면 클라이언트가 ResourceManager에게 일련의 요청을 순서대로 보내야 합니다. 이 과정에서 어플리케이션을 식별할 수 있는 ID를 발급받고, 실행 계획을 제출하며, 실행 상태를 모니터링할 수 있는 정보를 얻게 됩니다.

1. **클라이언트가 Application ID를 요청합니다**  
   클라이언트가 YARN 클러스터에서 새로운 어플리케이션을 실행하려면 먼저 **Application ID**가 필요합니다.  
   이를 위해 `ClientRMService`의 `createNewApplication()` 메소드를 호출합니다.

   > 💡 예시:  
   > 사용자가 Spark 작업을 제출하면, Spark 클라이언트가 내부적으로 `createNewApplication()`을 호출하여 YARN에 새 작업 ID를 요청합니다.
   
   <br>
   
2. **YARN이 Application ID와 리소스 정보를 반환합니다**  
   `ClientRMService`는 요청을 받고, 새로운 Application ID와 함께 클러스터에서 사용 가능한 최대 리소스 정보를 포함한 `GetNewApplicationResponse` 객체를 클라이언트에 전달합니다.  
   예: 최대 사용 가능한 메모리, 최대 CPU 코어 수 등

   > 💡 예시:  
   > 클러스터 전체에서 한 컨테이너당 8GB 메모리, 4코어까지 할당 가능하다는 정보가 함께 전달됩니다.
   
   <br>
   
3. **클라이언트가 Application 제출 요청을 보냅니다**  
   클라이언트는 Application ID가 정상적으로 발급되었는지 확인한 후, `ClientRMService`의 `submitApplication()` 메소드를 호출하여 어플리케이션 실행 요청을 보냅니다.  
   이때 **ApplicationSubmissionContext** 객체를 파라미터로 전달합니다. 이 객체에는 실행에 필요한 다양한 정보가 포함되어 있습니다:

   - Application ID
   - Application Name (예: "ETL Job for 2025-06-03")
   - Queue name (예: "default", "etl")
   - Application Priority
   - 필요한 리소스 (메모리, CPU 등)
   - Application Master 실행 명령이 담긴 `ContainerLaunchContext`

   > 💡 예시:  
   > "spark-etl-job"이라는 작업을 `etl` 큐에 제출하며, 메모리 4GB, CPU 2코어, 실행 명령은 `spark-submit`으로 설정합니다.
   
   <br>
   
4. **클라이언트가 Application 상태를 요청합니다**  
   클라이언트는 어플리케이션이 정상적으로 등록되었는지 확인하기 위해 `getApplicationReport()`를 호출합니다.
   
   <br>
   
5. **ResourceManager가 ApplicationReport를 반환합니다**  
   ResourceManager는 요청에 따라 해당 어플리케이션의 상태 및 실행 정보를 담은 `ApplicationReport` 객체를 반환합니다.  
   이 보고서는 어플리케이션의 실행 상태를 모니터링할 때 유용합니다.

   **ApplicationReport에 포함된 정보:**

   | 항목 | 설명 |
   |------|------|
   | Application ID | 어플리케이션의 고유 ID |
   | User | 실행한 사용자 이름 |
   | Queue | 실행된 큐 이름 |
   | Application Name | 어플리케이션 이름 |
   | ApplicationMaster Host | ApplicationMaster가 실행 중인 노드 |
   | RPC Port | ApplicationMaster의 통신 포트 |
   | Tracking URL | 웹 UI에서 상태 추적 가능 링크 |
   | YarnApplicationState | 어플리케이션 상태 (예: SUBMITTED, RUNNING, FINISHED 등) |
   | Diagnostic Info | 오류 발생 시 메시지 |
   | Start Time | 어플리케이션 시작 시각 |
   | Client Token | 보안 설정이 활성화된 경우 인증 토큰 |

   > 💡 예시:  
   > 클라이언트는 이 정보를 주기적으로 조회해 작업 진행 상태나 실패 여부를 확인하고, 대시보드나 모니터링 시스템에서 시각화할 수 있습니다.

<br>
### 🐘 1.3.2. RM ▸ NM : Application Master 실행 요청

<img src="/assets/img/yarn6.png" alt="YARN" style="border: 2px solid skyblue; border-radius: 4px;" />

> 출처 : [YARN Application Master 실행 요청 단계](https://velog.io/@me529/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81-Part8-Ch3.-Yarn)

<br>
클라이언트가 어플리케이션 제출을 완료하면, 이제 YARN은 클러스터 내부에서 **ApplicationMaster**를 실행하여 본격적으로 어플리케이션 실행을 시작합니다. 이 과정은 ResourceManager의 내부 구성 요소들과 NodeManager 간의 협업을 통해 이뤄집니다.

1. **RMAppManager가 ApplicationMaster 실행을 위한 컨테이너를 요청합니다**  
   ResourceManager의 구성 요소 중 하나인 `RMAppManager`는 YARN 내부 스케줄러에게 어플리케이션 등록 요청과 함께 ApplicationMaster 실행을 위한 컨테이너 할당을 요청합니다.
   
   <br>
   
2. **ApplicationAttemptId가 큐에 등록됩니다**  
   YARN은 해당 어플리케이션 실행 시도를 고유하게 식별하기 위해 `ApplicationAttemptId`를 생성하고, 이를 큐에 등록합니다.  
   그리고 `RMAppAttemptEventType.ATTEMPT_ADDED` 이벤트를 발생시켜 등록이 완료되었음을 알려줍니다.

   > 💡 예시:  
   > 하나의 Spark 작업이 실패 후 재시도되는 경우, 새로운 ApplicationAttemptId가 생성되어 큐에 다시 등록됩니다.
   
   <br>
   
3. **스케줄러에게 컨테이너 할당을 요청합니다**  
   RMAppManager는 스케줄러에게 ApplicationMaster 실행에 필요한 컨테이너를 할당해달라고 요청합니다.
   
   <br>
   
4. **스케줄러가 컨테이너 할당 후 START 이벤트를 발생시킵니다**  
   스케줄러는 적절한 NodeManager에서 사용할 수 있는 리소스를 기반으로 컨테이너를 할당하고, ApplicationMaster 실행을 위한 `RMContainerEventType.START` 이벤트를 발생시킵니다.
   
   <br>
   
5. **RMAppManager가 ApplicationMasterLauncher를 실행합니다**  
   컨테이너가 할당되면, RMAppManager는 `ApplicationMasterLauncher`를 통해 실제 ApplicationMaster 실행을 시작합니다.
   
   <br>
   
6. **ApplicationMasterLauncher가 AMLauncher를 실행합니다**  
   `ApplicationMasterLauncher`는 내부적으로 `AMLauncher`를 구동하여 ApplicationMaster를 실행합니다.
   
   <br>
   
7. **AMLauncher가 NodeManager에 ApplicationMaster 실행을 요청합니다**  
   AMLauncher는 컨테이너 실행에 필요한 정보를 담은 `ContainerLaunchContext`를 준비하고, 이를 NodeManager에게 전달하여 컨테이너 실행을 요청합니다.

   **ContainerLaunchContext에 포함되는 정보:**

   | 항목 | 설명 |
   |------|------|
   | ContainerId | 컨테이너 고유 식별자 |
   | Resource | 할당된 메모리 및 CPU 등 리소스 |
   | User | 해당 컨테이너에 할당된 사용자 |
   | Security Tokens | 보안이 활성화된 경우 필요한 토큰 |
   | LocalResources | 실행에 필요한 jar, 바이너리, shared-objects, side files 등 |
   | Service Data | (선택) 어플리케이션 전용 서비스 데이터 |
   | Environment Variables | 실행 시 필요한 환경 변수 |
   | Launch Command | 컨테이너 실행 명령어 |
   | Retry Strategy | 컨테이너가 실패할 경우 재시도 전략 |

   > 💡 예시:  
   > Spark의 ApplicationMaster는 내부적으로 `spark-submit` 명령을 launch command로 전달하고, 실행에 필요한 `spark-assembly.jar` 등을 local resource로 포함합니다.
   
   <br>
   
8. **NodeManager가 컨테이너를 실행하고 결과를 반환합니다**  
   NodeManager는 전달받은 컨텍스트를 기반으로 컨테이너를 실행하고, 그 결과를 `StartContainersResponse`에 담아 AMLauncher에 반환합니다.

   > 💡 예시:  
   > 컨테이너가 정상적으로 실행되면, NodeManager는 `SUCCESS` 상태와 함께 컨테이너의 상태 정보를 포함한 응답을 반환합니다.


<br>
### 🐘 1.3.3. NM ▸ AM : JVM으로 Application Master 실행
NodeManager는 ResourceManager의 요청에 따라 지정된 컨테이너 안에서 ApplicationMaster를 실행합니다.  
이 과정은 실제로 **새로운 JVM 프로세스를 생성**해서 ApplicationMaster를 구동시키는 방식으로 이루어집니다.

ApplicationMaster를 실행하기 위해 NodeManager는 `ContainerLaunchContext`라는 실행 컨텍스트 정보를 사용합니다.  
이 컨텍스트에는 다음과 같은 실행에 필요한 정보들이 포함되어 있습니다:

- 컨테이너 ID
- 할당된 리소스 정보 (메모리, CPU 등)
- 컨테이너를 실행할 사용자 정보
- 바이너리나 JAR 파일 등 로컬 리소스
- 환경 변수
- 실행 명령어 (command)
- 보안 토큰 (필요한 경우)
- 실패 시 재시도 전략 등

<br>
> 💡 예시:  
> `ContainerLaunchContext`의 command 필드에는 `"java -Xmx1024m com.example.MyAppMaster"` 와 같은 실제 실행 명령어가 설정됩니다.

<br>
NodeManager는 이 컨텍스트를 바탕으로 JVM을 생성하고 ApplicationMaster를 구동합니다.  
ApplicationMaster가 정상적으로 실행되면, 이후 단계에서 ResourceManager에 등록 요청을 하게 됩니다.

<br>
### 🐘 1.3.4. AM ▸ RM : 필요한 리소스 요청, NM 목록 반환 및 Application Master 등록

NodeManager가 ApplicationMaster를 성공적으로 실행한 후에는, 해당 ApplicationMaster가 반드시 ResourceManager에 **등록**되어야 합니다.  
이 과정을 통해 ResourceManager는 ApplicationMaster에게 적절한 자원을 할당하거나 상태를 모니터링할 수 있게 됩니다.

등록 과정에 들어가기 전에, 먼저 ApplicationMaster와 ResourceManager가 어떻게 통신하는지 간단히 살펴보겠습니다.

두 컴포넌트는 **ApplicationMasterProtocol**이라는 인터페이스를 통해 상호작용합니다.  
YARN은 이 인터페이스를 구현한 기본 클라이언트로 다음 두 가지를 제공합니다.

- `AMRMClient`
- `AMRMClientAsync`

필요하다면 사용자가 직접 `ApplicationMasterProtocol`을 구현하여 커스터마이징할 수도 있습니다.

**ApplicationMasterProtocol**에서 제공하는 주요 메소드는 아래와 같습니다:

| 메소드 | 설명 |
|--------|------|
| `registerApplicationMaster(request)` | ApplicationMaster를 ResourceManager에 등록 |
| `allocate(request)` | 자원 요청 및 상태 보고 (heartbeat 포함) |
| `finishApplicationMaster(request)` | 실행 완료 후 종료 처리 요청 |

이제 본격적으로 ApplicationMaster가 어떻게 등록되는지 구체적인 과정을 살펴보겠습니다.

<img src="/assets/img/yarn7.png" alt="YARN" style="border: 2px solid skyblue; border-radius: 4px;" />

> 출처 : [YARN Application Master 등록 단계](https://velog.io/@me529/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81-Part8-Ch3.-Yarn)

<br>

1. **ApplicationMaster가 ResourceManager에 등록 요청**

   ApplicationMaster는 `registerApplicationMaster()` 메소드를 호출하여 ResourceManager에 자신을 등록합니다.

   이때 전달되는 `RegisterApplicationMasterRequest` 안에는 다음 정보가 포함됩니다.

   - ApplicationMaster가 실행 중인 **호스트 이름**
   - **RPC 포트**
   - 사용자가 어플리케이션 상태를 확인할 수 있는 **Tracking URL**

   > 💡 예시:  
   > Spark의 ApplicationMaster는 `spark://worker-node1:7077` 같은 RPC 포트와 Web UI 주소를 등록합니다.

   <br>

2. **ResourceManager가 등록 처리 및 응답**

   ResourceManager의 `ApplicationMasterService`는 등록 요청을 받은 후, 해당 ApplicationMaster를 내부 목록에 추가합니다. 그리고 `RegisterApplicationMasterResponse`를 반환합니다.

   이 응답은 `AllocateResponse` 형태로 구성되며 다음 정보를 포함합니다.

   - 중복 응답 방지를 위한 Response ID
   - AM에게 전달되는 명령어 (e.g., 재시작, 종료 등)
   - 새롭게 할당된 컨테이너 목록
   - 종료된 컨테이너들의 상태
   - 클러스터에서 현재 사용 가능한 리소스 양 (headroom)
   - 변경된 노드 상태 목록
   - 전체 노드 수
   - 반환 요청된 리소스
   - (보안 활성화 시) AMRMToken
   - 리소스가 증가 또는 감소된 컨테이너 목록

   <br>

3. **ApplicationMaster가 allocate 메소드로 리소스를 요청**

   ApplicationMaster는 이후 주기적으로 `allocate()` 메소드를 호출하여 다음을 수행합니다.

   - 필요한 리소스 요청 (예: 컨테이너 개수, 위치, 사양 등)
   - 어플리케이션의 현재 진행률 보고
   - 사용하지 않는 컨테이너 반환
   - 실행 중인 컨테이너의 리소스 변경 요청

   > 💡 `allocate()` 메소드는 상태 유지를 위한 **heartbeat** 역할도 수행합니다.  
   > AM은 필요한 컨테이너를 한 번에 못 받아도 이 메소드를 통해 점진적으로 리소스를 받을 수 있습니다.  
   > `AMRMClientAsync`를 사용할 경우 기본 호출 간격은 1초입니다.

   <br>

4. **ResourceManager가 요청을 스케줄러에게 위임**

   `ApplicationMasterService`는 받은 allocate 요청을 내부 **스케줄러**에게 전달합니다.  
   스케줄러는 요청된 자원이 가용한지 확인하고, 가능할 경우 해당 리소스를 할당합니다.  
   결과는 다시 `AllocateResponse`에 담겨 ApplicationMaster로 반환됩니다.

   > 💡 예시:  
   > 스케줄러는 사용자의 큐, 우선순위, 현재 클러스터 상태 등을 기반으로 자원 할당 여부를 판단합니다.

<br>
### 🐘 1.3.5. AM ▸ NM : 컨테이너 실행

<img src="/assets/img/yarn8.png" alt="YARN" style="border: 2px solid skyblue; border-radius: 4px;" />

> 출처 : [YARN 컨테이너 실행 및 Application 실행 단계](https://velog.io/@me529/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81-Part8-Ch3.-Yarn)

<br>

ApplicationMaster는 자신이 할당받은 컨테이너에서 어플리케이션을 실행해야 합니다. 이를 위해 ApplicationMaster는 NodeManager와 상호작용합니다.

1. ApplicationMaster의 클라이언트는 NodeManager에게 컨테이너 실행을 요청합니다.  
   이때 사용되는 `StartContainersRequest`에는 다음 정보가 포함되어 있습니다.  
   - 할당된 리소스 (메모리, CPU 등)  
   - 보안 토큰 (보안이 활성화된 경우)  
   - 컨테이너를 시작하기 위한 실행 명령어  
   - 프로세스 환경 변수  
   - 필요한 바이너리, JAR 파일, 공유 라이브러리 등  

   <br>

2. NodeManager의 `ContainerManager`는 `startContainer` 메소드를 처리합니다.  
   `ContainerManager`는 ApplicationMaster가 요청한 대로 컨테이너를 실행하고, 실행 결과를 `StartContainersResponse`로 반환합니다.  
   `StartContainersResponse`는 다음과 같은 정보를 포함합니다.  
   - `getAllServicesMetaData()` : 실행된 서비스의 메타데이터  
   - `getFailedRequests()` : 실패한 컨테이너 실행 요청 목록  
   - `getSuccessfullyStartedContainers()` : 성공적으로 시작된 컨테이너 목록

<br>
> 💡 예시:  
> ApplicationMaster가 NodeManager에 컨테이너 실행을 요청했을 때,  
> NodeManager가 할당된 메모리 4GB, CPU 2개를 가진 컨테이너를 시작하고, 실행 명령어로 `java -jar app.jar`를 실행합니다.  
> 실행이 성공하면 `StartContainersResponse`에서 성공적으로 시작된 컨테이너 목록에 해당 컨테이너가 포함되어 클라이언트에 반환됩니다.

<br>
### 🐘 1.3.6. NM ▸ Container : Application 실행

사실 위 컨테이너 실행 과정과 Application 실행 과정을 하나로 통합하여 봐도 좋습니다.

1. 컨테이너가 성공적으로 실행된 이후, ApplicationMaster는 `getContainerStatuses()`를 주기적으로 호출하여 각 컨테이너의 어플리케이션 상태를 모니터링합니다.

2. NodeManager의 `ContainerManager`는 ApplicationMaster가 요청한 컨테이너 상태를 `GetContainerStatusesResponse`로 반환합니다. 이 응답에는 컨테이너의 상태 정보가 포함되어 있습니다.

<br>
> 💡 예시:  
> ApplicationMaster가 주기적으로 컨테이너 상태를 조회할 때,  
> NodeManager는 컨테이너가 현재 `RUNNING` 상태임을 응답하고, 만약 오류가 발생했으면 오류 정보를 포함하여 전달합니다.  
> 이를 통해 ApplicationMaster는 각 컨테이너의 상태를 실시간으로 관리할 수 있습니다.

<br>

### 🐘 1.3.7. Application Master 종료

<img src="/assets/img/yarn9.png" alt="YARN" style="border: 2px solid skyblue; border-radius: 4px;" />

> 출처 : [YARN Application Master 종료 단계](https://velog.io/@me529/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81-Part8-Ch3.-Yarn)

<br>
컨테이너에서 실행했던 어플리케이션들이 종료되면, ApplicationMaster도 종료되어야 합니다.  
ApplicationMaster가 종료되면 하나의 어플리케이션 라이프사이클이 완전히 종료되는 것입니다.

1. ApplicationMaster의 클라이언트는 ResourceManager에게 ApplicationMaster 종료를 요청합니다.

2. ResourceManager의 ApplicationMasterService는 해당 ApplicationMaster를 클러스터에서 해제하고, `FinishApplicationMasterResponse`를 반환합니다.  
   - `FinishApplicationMasterResponse`에는 `getIsUnregistered()`라는 boolean 메소드가 있어서, 정상적인 해제와 종료 가능 여부를 알려줍니다.  
   - 만약 `getIsUnregistered()`가 true가 되기 전에 Application이 먼저 멈춘다면, ResourceManager는 해당 어플리케이션을 재시도합니다.

<br>

> 💡 예시:  
> 어떤 어플리케이션이 정상적으로 작업을 마치고 종료 신호를 보내면, ResourceManager는 이 요청을 받아 ApplicationMaster를 해제하고, `getIsUnregistered()`가 true인 응답을 반환합니다.  
> 반면, 작업이 갑자기 중단되거나 비정상 종료되면, ResourceManager는 재시도를 위해 ApplicationMaster를 다시 실행시킬 수 있습니다.

<br>
### 🐘 1.3.8. Auxiliary Service
YARN에서 **Auxiliary Service(보조 서비스)**는 NodeManager 간에 데이터를 주고받거나 서비스 제어를 가능하게 하는 기능입니다.

특히 하둡 맵리듀스 작업에서, **맵 태스크와 리듀스 태스크 사이의 데이터 전달 과정인 '셔플(Shuffle)'**을 원활히 수행하는 데 중요한 역할을 합니다.

맵 태스크는 각 노드의 NodeManager가 관리하는 컨테이너(Container) 내에서 실행됩니다. 맵 태스크가 작업한 중간 결과 데이터는 리듀스 태스크가 실행되는 다른 노드의 컨테이너로 전달되어야 합니다. 그런데, NodeManager는 컨테이너 내 애플리케이션이 종료되면 해당 컨테이너를 즉시 종료합니다. 만약 맵 태스크가 끝나면서 컨테이너가 종료되면, 맵 태스크가 만든 중간 데이터를 저장하거나 리듀스 태스크에 전달할 수 없게 됩니다. 그 결과, 리듀스 태스크가 필요한 데이터를 받지 못해 리듀스 작업을 수행할 수 없게 됩니다.

즉, 맵 태스크가 데이터를 전달하기 전에 컨테이너가 사라지면, 리듀스 작업을 수행할 수 없게 되어 전체 작업 흐름이 멈추게 됩니다.

이런 문제를 해결하기 위해 YARN은 Auxiliary Service를 통해 NodeManager 간에 데이터를 안정적으로 전송할 수 있는 별도의 서비스 채널을 제공합니다. 이를 통해 컨테이너가 종료되더라도 맵에서 리듀스로의 데이터 전달(셔플)이 원활히 이루어질 수 있습니다.

따라서 Auxiliary Service는 맵리듀스 작업의 핵심 단계인 셔플 작업을 성공적으로 수행하기 위한 필수적인 보조 기능입니다.

<img src="/assets/img/yarn10.png" alt="YARN" style="border: 2px solid skyblue; border-radius: 4px;" />

> 출처 : [YARN Application Master 종료 단계](https://velog.io/@me529/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81-Part8-Ch3.-Yarn)

<br>
**동작 과정은 아래와 같습니다.**

1. 클라이언트가 ResourceManager에게 어플리케이션 실행을 요청합니다.  

2. ResourceManager는 해당 어플리케이션의 ApplicationMaster를 실행합니다.  

3. YARN은 맵리듀스 어플리케이션의 ApplicationMaster로 `MRAppMaster`를 제공합니다.  
   - ResourceManager가 ApplicationMaster 실행을 요청하면 NodeManager가 컨테이너에서 `MRAppMaster`를 실행합니다.  
   
4. `MRAppMaster`는 다른 NodeManager에게 맵 태스크 실행을 요청합니다.  

5. NodeManager는 컨테이너에서 맵 태스크를 실행합니다.  

6. 맵 태스크가 수행한 결과는 셔플 과정을 통해 리듀스 태스크에 전달됩니다.  
   - 셔플(Shuffle)을 담당하는 기본 클래스는 하둡의 `mapred` 패키지에 구현된 `ShuffleHandler`입니다.
   
<br>
> 💡 **예시**  
> 예를 들어, 맵 태스크가 노드 A의 컨테이너에서 실행되고, 리듀스 태스크가 노드 B에서 실행될 때,  
> 노드 A의 NodeManager는 Auxiliary Service를 통해 중간 데이터를 안전하게 노드 B의 NodeManager로 전달합니다.  
> 이 과정에서 `ShuffleHandler`가 셔플 데이터를 중계하며, 맵과 리듀스 사이 데이터 전달이 끊기지 않도록 보장합니다.

<br>
### 🐘 1.3.9. Pluggable Shuffle 및 Pluggable Sort 설정

Pluggable Shuffle 및 Pluggable Sort는 이 Auxiliary Service 위에서 동작하는, 셔플 단계의 데이터 처리 방식을 사용자 정의(customize)할 수 있도록 하는 기능입니다.

즉, 셔플 데이터를 어떻게 수집(ShuffleConsumerPlugin)하고 정렬(MapOutputCollector)할지를 플러그인 형태로 바꿀 수 있습니다.

다시 말해,
**Auxiliary Service는 셔플 데이터를 전달하는 ‘기반 인프라’**이고, **Pluggable Shuffle/Sort는 그 위에서 작동하는 ‘동작 방식을 커스터마이징하는 기능’**입니다.

Auxiliary Service는 맵리듀스의 셔플과 정렬 동작을 다음 두 가지 방식으로 커스터마이징할 수 있습니다.

1. **Job 제출 시 설정**  
   - Job configuration에 인터페이스 구현체(예: `ShuffleConsumerPlugin`, `MapOutputCollector`)를 포함한 클래스를 지정합니다.  
   - 이 클래스들은 해당 Job 패키지 내에 위치해야 합니다.  
   - 예를 들어, job-conf.xml 혹은 프로그램 코드 내에서 관련 클래스를 지정하여 특정 셔플 동작을 커스터마이징합니다.


2. **yarn-site.xml 설정**  
   - 클러스터에 배포된 모든 노드의 yarn-site.xml 파일에 auxiliary-services 및 관련 서비스 설정을 추가합니다.
   - 이를 통해 클러스터 전체에 커스텀 Auxiliary Service를 적용할 수 있습니다.
   - 예를 들어 아래와 같습니다.
   
   ```
   <property>
       <name>yarn.app.mapreduce.aux-services</name>
       <value>shuffle</value>
   </property>
   <property>
       <name>yarn.app.mapreduce.aux-services.shuffle.class</name>
       <value>org.apache.hadoop.mapred.ShuffleHandler</value>
   </property>
   ```
<br>
> 💡 **예시**  
> 만약 특정 회사에서 자체 개발한 셔플 최적화 로직을 적용하고 싶다면,  
> Job 제출 시에 `ShuffleConsumerPlugin` 인터페이스를 구현한 클래스를 제공하거나,  
> 클러스터 전반에 적용할 경우 `yarn-site.xml`에 해당 `aux-service` 설정을 배포해 셔플 동작을 커스터마이징할 수 있습니다.

<br>
> **참고**  
> 해당 내용은 [YARN 공식 매뉴얼](https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/PluggableShuffleAndPluggableSort.html) 자료를 참고하였습니다.

<br>
# 🐘 2. YARN 아키텍처 심화

<img src="/assets/img/yarn11.png" alt="YARN" style="border: 2px solid skyblue; border-radius: 4px;" />

> 출처 : [YARN 심화 Resource Manager](https://velog.io/@me529/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81-Part8-Ch3.-Yarn)

<br>

<img src="/assets/img/yarn12.png" alt="YARN" style="border: 2px solid skyblue; border-radius: 4px;" />

> 출처 : [YARN 심화 Node Manager](https://velog.io/@me529/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81-Part8-Ch3.-Yarn)

<br>
<br>
<div align="center">◈</div>
<br>

# 🐘 3. YARN TimeLine Service V2

<br>
<br>
<div align="center">◈</div>
<br>

# 🐘 4. YARN 실습

<br>
<br>
<div align="center">◈</div>
<br>

# ✏️ 결론

<br>
<br>
<div align="center">◈</div>
<br>

# 📚 공부 참고 자료
---

[📑 1. 패스트 캠퍼스 - 한 번에 끝내는 데이터 엔지니어링 초격차 패키지 Online.](https://fastcampus.co.kr/)

[📑 2. 데이터엔지니어링 Yarn](https://velog.io/@me529/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81-Part8-Ch3.-Yarn)

[📑 3. Hadoop YARN 공식 매뉴얼](https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/PluggableShuffleAndPluggableSort.html)
