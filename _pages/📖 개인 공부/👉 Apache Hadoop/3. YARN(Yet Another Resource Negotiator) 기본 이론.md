---
title: "📘 YARN(Yet Another Resource Negotiator) 기본 이론"
tags:
    - Apache
    - Hadoop
    - Study
date: "2025-03-29"
thumbnail: "/assets/img/thumbnail/hadoop_basic_5.png"
bookmark: true
---

저번 포스팅에서는, 하둡의 핵심을 공부하였습니다.
이번에는 Hadoop V2 의 핵심인 YARN 에 대하여 공부한 내용을 포스팅 하도록 하겠습니다.

[📘 분산 시스템의 이해와 하둡의 등장 배경](https://jeondaehong.github.io/%F0%9F%93%96%20%EA%B0%9C%EC%9D%B8%20%EA%B3%B5%EB%B6%80/%F0%9F%91%89%20Apache%20Hadoop/1.%20%EB%B6%84%EC%82%B0%20%EC%8B%9C%EC%8A%A4%ED%85%9C%EC%9D%98%20%EC%9D%B4%ED%95%B4%EC%99%80%20%ED%95%98%EB%91%A1%EC%9D%98%20%EB%93%B1%EC%9E%A5%20%EB%B0%B0%EA%B2%BD.html)
[📘 하둡의 핵심 구성요소와 이론](https://jeondaehong.github.io/%F0%9F%93%96%20%EA%B0%9C%EC%9D%B8%20%EA%B3%B5%EB%B6%80/%F0%9F%91%89%20Apache%20Hadoop/2.%20%ED%95%98%EB%91%A1%EC%9D%98%20%ED%95%B5%EC%8B%AC%20%EA%B5%AC%EC%84%B1%EC%9A%94%EC%86%8C%EC%99%80%20%EC%9D%B4%EB%A1%A0.html)

<br>
<div align="center">◈</div>
<br>

# 🐘 1. YARN

2012년 이전, Hadoop을 이용해 대용량 처리를 하려면 MapReduce 모델을 기반으로 Java, Python, Ruby, 혹은 Pig 프레임워크를 사용해 개발해야 했습니다.  

하지만 Hadoop 2.0과 함께 등장한 Yarn은 MapReduce 프로그래밍 모델의 제약에서 벗어나, 다양한 멀티프로세싱 프로그램을 Hadoop 자원을 활용해 자유롭게 실행할 수 있게 해주었습니다.  

Yarn은 대용량 멀티프로세싱 처리에서 성능 향상과 유연한 실행 엔진(execution engine)을 제공한다는 점이 큰 장점입니다.

또한, 시간이 지나면서 Spark와 같은 다양한 분산 처리 프레임워크도 Yarn을 지원하게 되면서, 하둡 생태계에서 필수적인 리소스 관리 플랫폼으로 자리잡았습니다.

이러한 **YARN** 에 대하여, 지금부터 공부하고, 정리한 내용을 포스팅 해보도록 하겠습니다.

## 🐘 1.1. YARN 이란
---

<img src="/assets/img/yarn.png" alt="yarn" style="border: 2px solid skyblue; border-radius: 4px;" />

> 출처 : [YARN](https://bigdataschool.ru/blog/high-availability-of-hadoop-cluster-with-yarn.html)

<br>
Yarn은 Hadoop 2.0부터 도입된 클러스터 자원 관리 시스템(Resource Manager)입니다. 
이전 Hadoop 버전에서는 MapReduce라는 특정 프로그래밍 모델에만 의존해서 작업을 처리했는데, Yarn이 도입되면서 MapReduce에 국한되지 않고 다양한 종류의 분산 처리 작업을 실행할 수 있게 되었습니다.  

Yarn의 주요 역할은 클러스터 내 여러 노드의 자원을 효율적으로 관리하고 할당하여, 여러 애플리케이션이 동시에 안정적으로 실행될 수 있도록 조율하는 것입니다.  
즉, 여러 사용자가 다양한 작업을 동시에 실행해도 자원이 과도하게 집중되거나 낭비되지 않고, 적절하게 분배되어 전체 시스템의 효율성과 안정성을 높입니다.

Yarn은 또한 실행 환경을 유연하게 제공하여, Apache Spark, Flink, Tez 같은 다양한 분산 처리 프레임워크도 지원합니다.
이로 인해 빅데이터 처리의 범위가 단순한 배치 작업에서 실시간 스트리밍, 머신러닝, 그래프 처리 등으로 확장될 수 있었습니다.

<br>
### 🐘 1.1.1. Yarn의 대표적인 Use-Cases

Yarn이 등장하면서 Hadoop 기반 클러스터는 다음과 같은 장점과 가능성을 갖게 되었습니다.

- **🔍 배치 작업뿐 아니라 반복 작업과 실시간 스트리밍 처리 지원**  
  과거 Hadoop은 일괄 처리(batch processing)에 초점이 맞춰져 있었으나, Yarn 덕분에 지속적으로 데이터를 처리하는 스트리밍 작업과 주기적으로 반복 실행되는 작업들도 효율적으로 수행할 수 있게 되었습니다.  
  예를 들어, 실시간 로그 분석, 실시간 추천 시스템 등 웹 서비스에 필요한 빠른 데이터 처리가 가능해졌습니다.

- **🔍 클러스터 자원 활용률 극대화**  
  클러스터 내 자원을 한 애플리케이션이 모두 사용하지 않을 때, Yarn이 자원을 다른 애플리케이션에 재할당해 줌으로써 자원 활용률을 크게 향상시켰습니다.  
  이는 비용 절감과 클러스터 운영 효율을 높이는 데 중요한 역할을 합니다.

- **🔍 통합 클러스터 운영**  
  데이터 저장(HDFS), 데이터 처리(MapReduce, Spark 등), 데이터 조회 등 다양한 작업을 단일 클러스터에서 함께 처리할 수 있어 운영 관리가 간편해졌고, 데이터 이동에 필요한 시간과 비용도 줄일 수 있었습니다.

- **🔍 다양한 애플리케이션 동시 실행 지원**  
  한 클러스터에서 여러 종류의 애플리케이션이 동시에 안정적으로 실행되어, 여러 팀이나 서비스가 하나의 클러스터를 공유할 수 있습니다.
  
<br>
### 🐘 1.1.2. Container 란?

Docker 를 사용해보신 분들이라면, Container 의 개념을 어느정도 알고 계실 것입니다.

컨테이너는 소프트웨어 실행 환경을 하나의 패키지로 묶은 경량화된 가상화 기술입니다.  
여기에는 애플리케이션이 동작하는 데 필요한 코드, 라이브러리, 설정 파일 등이 포함되어 있어, 어디서든 동일한 환경에서 실행할 수 있습니다.

컨테이너 기술은 리눅스의 cgroup과 namespace 같은 기능을 활용하여, 하나의 물리적 서버 내에서도 각각의 컨테이너가 독립적으로 CPU, 메모리, 네트워크 자원을 사용할 수 있도록 격리합니다.  
이 격리 덕분에 서로 다른 컨테이너가 충돌하거나 간섭하지 않고, 안정적으로 동시에 실행될 수 있습니다.

컨테이너를 사용하는 주요 이유와 장점은 다음과 같습니다

- **📦 책임 분리(Separation of responsibility)**  
  개발자는 애플리케이션 코드와 그에 필요한 의존성만 신경 쓰면 되고, 배포 환경의 세부 사항(운영체제 버전, 라이브러리 충돌 등)은 크게 신경 쓸 필요가 없습니다.  
  덕분에 개발과 운영 간 협업이 원활해지고, 배포 및 테스트 과정에서 환경 문제로 인한 오류가 줄어듭니다.

- **📦 높은 이식성(Portability)**  
  컨테이너는 운영체제 수준에서 가상화되므로, 리눅스 기반 서버라면 어느 환경에서든 동일하게 실행할 수 있습니다.  
  단, 컨테이너가 정상적으로 작동하려면 해당 시스템이 컨테이너 실행을 지원해야 하며, 일반적으로 Linux/amd64 아키텍처에서 가장 원활합니다.

- **📦 애플리케이션 격리(Application isolation)**  
  컨테이너마다 CPU, 메모리, 스토리지, 네트워크 같은 자원을 할당하고 제한할 수 있기 때문에, 하나의 서버에서 여러 컨테이너가 동시에 실행되어도 서로 간섭 없이 안정적으로 작동합니다.  
  이는 시스템 안정성과 보안성을 높이는 데도 도움이 됩니다.

최근에는 **Kubernetes** 같은 오케스트레이션 도구와 함께 컨테이너를 사용해, 대규모 분산 시스템을 효율적으로 운영하는 사례가 많아지고 있습니다.  

**Yarn**도 이런 컨테이너 개념을 차용해 클러스터 자원 관리에 적용함으로써, 더욱 세밀하고 유연한 자원 할당과 작업 관리를 가능하게 합니다.


## 🐘 1.2. YARN 아키텍처
---

<img src="/assets/img/yarn2.png" alt="HDFS_and_YARN" style="border: 2px solid skyblue; border-radius: 4px;" />

> 출처 : [YARN 과 HDFS](https://seoyeonhwng.medium.com/hadoop-yarn-%EC%9D%B4%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80-3f209e901b3a)

<br>
Yarn 과 HDFS 자체는 완전히 독립적입니다.

Yarn은 CPU, 메모리와 같은 컴퓨팅 자원을 할당하고 관리하는 **자원 관리 소프트웨어**입니다.반면, HDFS는 분산 파일 시스템으로 **데이터 저장(스토리지)** 역할만 수행합니다. 따라서 **Yarn과 HDFS는 완전히 독립적인 시스템**이며, 서로 다른 역할을 맡고 있습니다. 

즉, Yarn은 저장소와 관계없이 클러스터의 컴퓨팅 자원 관리에 집중합니다.

그러한 Yarn 의 Architecture 는 크게 3가지 역할을 하는 컴포넌트로 구성 되어있습니다.

 - **Resource Manager:** 클러스터 전체 자원 관리 및 스케줄링

 - **Application Master:** 개별 애플리케이션의 자원 요청 및 관리

 - **Node Managers:** 각 노드에서 자원 할당 및 작업 실행

<img src="/assets/img/yarn3.png" alt="YARN_Architecture" style="border: 2px solid skyblue; border-radius: 4px;" />

> 출처 : [YARN 아키텍처](https://velog.io/@me529/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81-Part8-Ch3.-Yarn)

<br>
그럼 YARN 의 기본과 핵심 아키텍처를 하나하나 알아보도록 하겠습니다.

<br>
### 🐘 1.2.1. Resource Mananger (RM)
Resource Manager(RM)는 YARN 클러스터에서 가장 중요한 마스터 컴포넌트입니다.  

클러스터 내 모든 컴퓨팅 자원(CPU, 메모리 등)을 총괄 관리하는 **관리 본부** 역할을 합니다.  

예를 들어, 회사 내 여러 부서가 공유하는 컴퓨터 자원을 중앙에서 효율적으로 배분하는 총무팀이라고 생각할 수 있습니다.
 
RM은 클러스터 내에 존재하는 모든 노드들의 상태와 자원 정보를 알고 있으며, Rack Awareness(서로 물리적으로 가까운 서버 그룹 간의 네트워크 우선순위)도 고려해 자원 할당을 최적화합니다.  

RM 내부에는 여러 서비스가 존재하지만, 그 중에서도 Scheduler가 핵심입니다. Scheduler는 각 애플리케이션이 요청한 자원 요구사항을 분석해, 누가 언제 얼마나 자원을 쓸지 결정합니다.  

대표적인 Scheduler로는 Capacity Scheduler, Fair Scheduler 등이 있습니다. 이들은 클러스터 자원을 공평하게 나누거나, 우선순위에 따라 할당하는 정책을 구현합니다. 이렇게 RM은 단순히 자원을 할당하는 역할뿐 아니라, 리소스 할당 정책을 적용하고, 자원 사용 현황을 모니터링하며 장애 대응을 위한 정보도 관리합니다.  

<br>
### 🐘 1.2.2. Application Mananger (AM)
Application Master(AM)는 YARN에서 실행되는 각각의 애플리케이션에 대해 별도로 실행되는 매니저 프로세스입니다.  

클러스터 전체를 관리하는 RM과 달리, AM은 특정 애플리케이션 단위의 **프로젝트 팀장** 역할을 합니다.

애플리케이션이 시작될 때 가장 먼저 실행되는 컨테이너(Container)로서, 자신의 작업에 필요한 리소스를 RM에 요청하고 할당받은 자원을 활용해 작업을 수행합니다.
  
예를 들어, **데이터 분석 작업 팀**의 팀장이라면, RM에게 ‘CPU 4개, 메모리 8GB를 달라’고 요청하고, 할당받으면 그 자원을 기반으로 작업을 진행하는 식입니다.

AM은 할당받은 자원 내에서 작업 단위를 관리하고, 필요에 따라 추가 자원 요청, 작업 상태 모니터링, 오류 처리 등을 담당합니다. AM이 실패하면 해당 애플리케이션 작업 자체가 실패할 수 있으므로 안정성과 복구 메커니즘도 중요합니다. 또한 AM은 각기 다른 분산 처리 프레임워크(예: MapReduce, Spark, Tez 등)에 맞게 구현되며, 프레임워크별로 자원 관리 및 작업 조정 방법이 다를 수 있습니다. 

<br>
### 🐘 1.2.3. Node Manager
Node Manager 는 클러스터 내 여러 노드(서버)에서 실행되는 에이전트입니다. 
 
각 Node Manager 는 ‘서버 담당 기술자’로 비유할 수 있으며, 해당 노드의 상태와 리소스를 RM에 주기적으로 보고(heartbeat)합니다.  

Node Manager 는 CPU 코어 수, 사용 가능한 메모리 양 등 자신의 노드 자원 용량(capacity)을 관리합니다. 

YARN Scheduler가 할당을 결정하면 NM은 해당 자원 일부를 컨테이너(Container) 단위로 배정하고, 클라이언트가 제출한 작업을 실행합니다. 
 
컨테이너는 리소스 격리를 위해 독립적인 실행 공간을 제공하며, CPU, 메모리, 네트워크 사용량을 제한할 수 있습니다. 또한 NM은 자원 사용량 모니터링, 컨테이너 시작 및 종료, 장애 감지와 복구 등 작업을 수행합니다.  

Node Manager 는 Resource Mananger 의 지시에 따라 컨테이너 상태를 주기적으로 보고하며, RM은 이를 통해 클러스터 자원 현황을 정확히 파악할 수 있습니다.  

NodeManager 내부에는 Container Manager, NodeStatusUpdater, NodeHealthChecker 등의 서비스가 있어, 각각 컨테이너 관리, 상태 업데이트, 노드 건강 체크 기능을 수행합니다.  

이처럼 NM은 클러스터 내 물리적 하드웨어 자원을 추상화하여 YARN 애플리케이션에 안정적으로 제공하는 역할을 합니다.

<br>
## 🐘 1.3. YARN 의 작업 흐름
---

<img src="/assets/img/yarn4.png" alt="YARN" style="border: 2px solid skyblue; border-radius: 4px;" />

> 출처 : [YARN 전체적인 흐름](https://velog.io/@me529/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81-Part8-Ch3.-Yarn)

작업의 흐름을 먼저 간단히 정리하고, 세부적으로 들여다보겠습니다.

1. **클라이언트가 어플리케이션 실행 요청을 보냅니다.**  
   - 사용자가 작업을 실행하고 싶을 때, 클라이언트 프로그램이 Yarn API를 통해 실행 요청을 합니다.  
   - ResourceManager가 이 요청을 받고 유효하다고 판단하면, 클라이언트에게 고유한 Application ID(작업 식별 번호)를 할당합니다.

2. **ResourceManager가 NodeManager에게 Application Master 실행을 요청합니다.**  
   - ResourceManager는 클러스터 내 노드들 중 하나의 NodeManager에게 Application Master라는 특별한 프로그램을 실행하라고 지시합니다.  
   - Application Master는 해당 어플리케이션의 작업 진행을 총괄 관리하는 역할을 합니다.

3. **NodeManager가 컨테이너 안에서 Application Master를 실행합니다.**  
   - NodeManager는 ResourceManager의 요청을 받고, 새 컨테이너(작업 실행 공간)를 만들어 JVM(Java Virtual Machine)을 실행하여 Application Master를 띄웁니다.

4. **Application Master가 ResourceManager에게 필요한 리소스를 요청합니다.**  
   - Application Master는 작업에 필요한 리소스(메모리, CPU, 네트워크, 컨테이너 개수 등)를 ResourceManager에 요청합니다.  
   - ResourceManager는 클러스터 전체 리소스 상황을 파악해, 사용할 수 있는 NodeManager 목록을 Application Master에 전달합니다.

5. **Application Master가 할당받은 NodeManager에 컨테이너 실행을 요청합니다.**  
   - 받은 NodeManager들에게 실제 작업을 수행할 컨테이너 실행을 요청합니다.

6. **NodeManager들이 컨테이너를 실행하고 어플리케이션을 동작시킵니다.**  
   - NodeManager들은 각 컨테이너 안에 JVM을 새로 띄워 어플리케이션을 실행합니다.  
   - 작업이 끝나면 Application Master도 종료되고, ResourceManager는 종료된 Application Master가 사용하던 자원을 해제합니다.

<br>
이제 좀 더 세부적으로 들여다보겠습니다.

### 🐘 1.3.1. Application 실행 요청

<img src="/assets/img/yarn5.png" alt="YARN" style="border: 2px solid skyblue; border-radius: 4px;" />

> 출처 : [YARN Application 실행 요청 단계](https://velog.io/@me529/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81-Part8-Ch3.-Yarn)

<br>
YARN 클러스터에서 어플리케이션을 실행하려면 클라이언트가 ResourceManager에게 일련의 요청을 순서대로 보내야 합니다. 이 과정에서 어플리케이션을 식별할 수 있는 ID를 발급받고, 실행 계획을 제출하며, 실행 상태를 모니터링할 수 있는 정보를 얻게 됩니다.

1. **클라이언트가 Application ID를 요청합니다**  
   클라이언트가 YARN 클러스터에서 새로운 어플리케이션을 실행하려면 먼저 **Application ID**가 필요합니다.  
   이를 위해 `ClientRMService`의 `createNewApplication()` 메소드를 호출합니다.

   > 💡 예시:  
   > 사용자가 Spark 작업을 제출하면, Spark 클라이언트가 내부적으로 `createNewApplication()`을 호출하여 YARN에 새 작업 ID를 요청합니다.

<br>
2. **YARN이 Application ID와 리소스 정보를 반환합니다**  
   `ClientRMService`는 요청을 받고, 새로운 Application ID와 함께 클러스터에서 사용 가능한 최대 리소스 정보를 포함한 `GetNewApplicationResponse` 객체를 클라이언트에 전달합니다.  
   예: 최대 사용 가능한 메모리, 최대 CPU 코어 수 등

   > 💡 예시:  
   > 클러스터 전체에서 한 컨테이너당 8GB 메모리, 4코어까지 할당 가능하다는 정보가 함께 전달됩니다.

<br>
3. **클라이언트가 Application 제출 요청을 보냅니다**  
   클라이언트는 Application ID가 정상적으로 발급되었는지 확인한 후, `ClientRMService`의 `submitApplication()` 메소드를 호출하여 어플리케이션 실행 요청을 보냅니다.  
   이때 **ApplicationSubmissionContext** 객체를 파라미터로 전달합니다. 이 객체에는 실행에 필요한 다양한 정보가 포함되어 있습니다:

   - Application ID
   - Application Name (예: "ETL Job for 2025-06-03")
   - Queue name (예: "default", "etl")
   - Application Priority
   - 필요한 리소스 (메모리, CPU 등)
   - Application Master 실행 명령이 담긴 `ContainerLaunchContext`

   > 💡 예시:  
   > "spark-etl-job"이라는 작업을 `etl` 큐에 제출하며, 메모리 4GB, CPU 2코어, 실행 명령은 `spark-submit`으로 설정합니다.

<br>
4. **클라이언트가 Application 상태를 요청합니다**  
   클라이언트는 어플리케이션이 정상적으로 등록되었는지 확인하기 위해 `getApplicationReport()`를 호출합니다.

<br>
5. **ResourceManager가 ApplicationReport를 반환합니다**  
   ResourceManager는 요청에 따라 해당 어플리케이션의 상태 및 실행 정보를 담은 `ApplicationReport` 객체를 반환합니다.  
   이 보고서는 어플리케이션의 실행 상태를 모니터링할 때 유용합니다.

   **ApplicationReport에 포함된 정보:**

   | 항목 | 설명 |
   |------|------|
   | Application ID | 어플리케이션의 고유 ID |
   | User | 실행한 사용자 이름 |
   | Queue | 실행된 큐 이름 |
   | Application Name | 어플리케이션 이름 |
   | ApplicationMaster Host | ApplicationMaster가 실행 중인 노드 |
   | RPC Port | ApplicationMaster의 통신 포트 |
   | Tracking URL | 웹 UI에서 상태 추적 가능 링크 |
   | YarnApplicationState | 어플리케이션 상태 (예: SUBMITTED, RUNNING, FINISHED 등) |
   | Diagnostic Info | 오류 발생 시 메시지 |
   | Start Time | 어플리케이션 시작 시각 |
   | Client Token | 보안 설정이 활성화된 경우 인증 토큰 |

   > 💡 예시:  
   > 클라이언트는 이 정보를 주기적으로 조회해 작업 진행 상태나 실패 여부를 확인하고, 대시보드나 모니터링 시스템에서 시각화할 수 있습니다.

<br>
### 🐘 1.3.2. Application Master 실행 요청

### 🐘 1.3.3. Application Master 등록

### 🐘 1.3.4. 컨테이너 실행

### 🐘 1.3.5. Application Master 종료

### 🐘 1.3.6. Auxiliary Service

<br>
<br>
<div align="center">◈</div>
<br>

# 🐘 2. YARN TimeLine Service V2

<br>
<br>
<div align="center">◈</div>
<br>

# 🐘 3. YARN 실습

<br>
<br>
<div align="center">◈</div>
<br>

# ✏️ 결론

<br>
<br>
<div align="center">◈</div>
<br>

# 📚 공부 참고 자료
---

[📑 1. 패스트 캠퍼스 - 한 번에 끝내는 데이터 엔지니어링 초격차 패키지 Online.](https://fastcampus.co.kr/)

[📑 2. 데이터엔지니어링 Yarn](https://velog.io/@me529/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81-Part8-Ch3.-Yarn)

